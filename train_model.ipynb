{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cb0eca41-390a-465f-abdd-607025e484ac",
      "metadata": {
        "id": "cb0eca41-390a-465f-abdd-607025e484ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.21.0 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (1.21.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.21.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1de25582",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (2.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from tensorflow) (2.3.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast>=0.2.1 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: libclang>=9.0.1 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from tensorflow) (1.21.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from tensorflow) (49.2.1)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from tensorflow) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from tensorflow) (0.31.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.43.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.9)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.32.5)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: wheel>=0.26 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (2.0.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2025.11.12)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (8.7.0)\n",
            "Requirement already satisfied: zipp>=3.20 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\umarly\\documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "78fd5829-b633-4f8c-91b9-18bd7a31dff0",
      "metadata": {
        "id": "78fd5829-b633-4f8c-91b9-18bd7a31dff0"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'labels.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m num_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     30\u001b[0m path_to_works \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 31\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     33\u001b[0m     vocab \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(file\u001b[38;5;241m.\u001b[39mread())\n",
            "File \u001b[1;32mc:\\Users\\umarly\\Documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\umarly\\Documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:586\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    571\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    572\u001b[0m     dialect,\n\u001b[0;32m    573\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    583\u001b[0m )\n\u001b[0;32m    584\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\umarly\\Documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:482\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    479\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[1;32mc:\\Users\\umarly\\Documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:811\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwds:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 811\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\umarly\\Documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1040\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1037\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown engine: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (valid options are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1038\u001b[0m     )\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;66;03m# error: Too many arguments for \"ParserBase\"\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n",
            "File \u001b[1;32mc:\\Users\\umarly\\Documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:51\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     48\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musecols\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musecols\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# open handles\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_handles\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Have to pass int, would break tests using TextReader directly otherwise :(\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\umarly\\Documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:222\u001b[0m, in \u001b[0;36mParserBase._open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_handles\u001b[39m(\u001b[38;5;28mself\u001b[39m, src: FilePathOrBuffer, kwds: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03m    Let the readers open IOHandles after they are done with their potential raises.\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\umarly\\Documents\\russian-handwritten-text-recognition\\venv\\lib\\site-packages\\pandas\\io\\common.py:702\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    698\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    701\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 702\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    710\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    711\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'labels.csv'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense,LSTM\n",
        "import tensorflow.keras.layers as layers\n",
        "from tensorflow.keras.layers.experimental.preprocessing import  StringLookup\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "#Prepare data\n",
        "from sklearn.model_selection import train_test_split\n",
        "def load_image(path):\n",
        "\n",
        "    img = tf.io.read_file(\"images/\" + path)\n",
        "    # 2. Decode and convert to grayscale\n",
        "    img = tf.io.decode_image(img, channels=3)\n",
        "    # 3. Convert to float32 in [0, 1] range\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    # 4. Resize to the desired size\n",
        "    img = tf.image.resize(img, [50, 200])\n",
        "    # 5. Transpose the image because we want the time\n",
        "    # dimension to correspond to the width of the image.\n",
        "    img = tf.transpose(img, perm=[1, 0, 2])\n",
        "    img = img.numpy()\n",
        "    return img\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "num_labels = []\n",
        "path_to_works = []\n",
        "data = pd.read_csv(\"labels.csv\")\n",
        "with open(\"vocab.txt\", 'r') as file:\n",
        "    vocab = json.loads(file.read())\n",
        "\n",
        "filenames = data[\"file_name\"]\n",
        "\n",
        "labels = data[\"text\"]\n",
        "\n",
        "characters = set(char for label in labels for char in label)\n",
        "max_length = max([len(label) for label in labels])\n",
        "\n",
        "#merge new and old dictionary\n",
        "for c in characters:\n",
        "    if c not in vocab:\n",
        "        vocab.append(c)\n",
        "\n",
        "with open(\"vocab.txt\", 'w') as file:\n",
        "    file.write(json.dumps(vocab))\n",
        "\n",
        "\n",
        "#save data to several files  \n",
        "for path in filenames[:20000]:\n",
        "    images.append(load_image(path))\n",
        "np.savez_compressed(\"x1.npz\",a=images, b=np.array([]))\n",
        "\n",
        "for path in filenames[20000:40000]:\n",
        "    images.append(load_image(path))\n",
        "np.savez_compressed(\"x2.npz\",a=images, b=np.array([]))\n",
        "\n",
        "for path in filenames[40000:60000]:\n",
        "    images.append(load_image(path))\n",
        "np.savez_compressed(\"x3.npz\",a=images, b=np.array([]))\n",
        "\n",
        "for path in filenames[60000:80000]:\n",
        "    images.append(load_image(path))\n",
        "np.savez_compressed(\"x4.npz\",a=images, b=np.array([]))\n",
        "\n",
        "\n",
        "for path in filenames[80000:100000]:\n",
        "    images.append(load_image(path))\n",
        "np.savez_compressed(\"x5.npz\",a=images, b=np.array([]))\n",
        "\n",
        "for path in filenames[100000:120000]:\n",
        "    images.append(load_image(path))\n",
        "np.savez_compressed(\"x6.npz\",a=images, b=np.array([]))\n",
        "\n",
        "for path in filenames[120000:140000]:\n",
        "    images.append(load_image(path))\n",
        "np.savez_compressed(\"x7.npz\",a=images, b=np.array([]))\n",
        "\n",
        "for path in filenames[160000:]:\n",
        "    images.append(load_image(path))\n",
        "np.savez_compressed(\"x8.npz\",a=images, b=np.array([]))\n",
        "\n",
        "#make all words be equal length\n",
        "def proccess(lbl):\n",
        "\n",
        "    if len(lbl) < max_length:\n",
        "        for i in range(0, max_length - len(lbl)):\n",
        "            lbl+=' '\n",
        "    return lbl\n",
        "labels = list(map(proccess, labels))\n",
        "\n",
        "#tokenize\n",
        "for label in labels:\n",
        "\n",
        "\n",
        "    if label !=\".DS_Store\":\n",
        "        nums = []\n",
        "\n",
        "        for c in label.replace(\".png\",'').replace(\".jpg\",''):\n",
        "            nums.append(vocab.index(c))\n",
        "\n",
        "\n",
        "\n",
        "        num_labels.append(nums)\n",
        "num_labels = np.array(num_labels)\n",
        "np.save(\"labels.npy\", num_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54d714f2-872c-408c-9b0c-82ef5ac01f99",
      "metadata": {
        "id": "54d714f2-872c-408c-9b0c-82ef5ac01f99"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "x1 = np.load(\"x1.npz\")['a']\n",
        "\n",
        "x2 = np.load(\"x2.npz\")['a']\n",
        "\n",
        "x3 = np.load(\"x3.npz\")['a']\n",
        "\n",
        "x4 = np.load(\"x4.npz\")['a']\n",
        "\n",
        "x5 = np.load(\"x5.npz\")['a']\n",
        "\n",
        "x6 = np.load(\"x6.npz\")['a']\n",
        "\n",
        "x7 = np.load(\"x7.npz\")['a']\n",
        "\n",
        "x8 = np.load(\"x8.npz\")['a']\n",
        "\n",
        "\n",
        "\n",
        "labels = np.load(\"labels.npy\").astype(\"int64\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f11aee3-506e-406b-8665-2a6577f3fead",
      "metadata": {
        "id": "0f11aee3-506e-406b-8665-2a6577f3fead"
      },
      "outputs": [],
      "source": [
        "import  tensorflow.keras.backend as K\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense,LSTM\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.layers as layers\n",
        "\n",
        "import tensorflow.keras\n",
        "import os\n",
        "from tensorflow.keras.layers import Dense, Input, Conv2D, MaxPooling2D, Bidirectional, LSTM, Reshape, Dropout\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class CTCLayer(layers.Layer):\n",
        "    def __init__(self, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.loss_fn = tf.keras.backend.ctc_batch_cost\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        # Compute the training-time loss value and add it\n",
        "        # to the layer using `self.add_loss()`.\n",
        "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
        "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
        "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
        "\n",
        "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "\n",
        "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
        "        self.add_loss(loss)\n",
        "\n",
        "        # At test time, just return the computed predictions\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "img_input = Input(shape=(200, 50, 3), name=\"image_input\",dtype=\"float32\" )\n",
        "lbl_input = Input(shape=(None,),dtype=\"float32\")\n",
        "\n",
        "x = layers.Conv2D(64,(3, 3), activation=\"relu\",padding=\"same\")(img_input)\n",
        "\n",
        "x = layers.Conv2D(64,(3, 3), activation=\"relu\",padding=\"same\")(x)\n",
        "\n",
        "x = layers.MaxPooling2D((2,2))(x)\n",
        "\n",
        "x = layers.BatchNormalization()(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x = layers.Conv2D(128,(3, 3), activation=\"relu\",padding=\"same\")(x)\n",
        "x = layers.Conv2D(128,(3, 3), activation=\"relu\",padding=\"same\")(x)\n",
        "\n",
        "\n",
        "\n",
        "x = layers.MaxPooling2D((2,2))(x)\n",
        "\n",
        "\n",
        "x = layers.BatchNormalization()(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x = layers.Conv2D(\n",
        "        64,\n",
        "        (3, 3),\n",
        "        activation=\"relu\",\n",
        "        kernel_initializer=\"he_normal\",\n",
        "        padding=\"same\",\n",
        "        name=\"Conv1\",\n",
        "    )(x)\n",
        "\n",
        "x = layers.BatchNormalization()(x)\n",
        "\n",
        "x = Reshape(((200 // 4), (50 // 4) * 64))(x)\n",
        "\n",
        "x = Dense(64, activation=\"relu\",kernel_initializer=\"he_normal\")(x)\n",
        "x = Dropout(0.4)(x)\n",
        "x = Bidirectional(LSTM(256, return_sequences=True, dropout=0.35))(x)\n",
        "x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.35), name=\"last_lstm\")(x)\n",
        "\n",
        "x = Dense(151, activation=\"softmax\", name=\"target_dense\")(x)\n",
        "output = CTCLayer()(lbl_input, x)\n",
        "\n",
        "model = Model([img_input, lbl_input], output)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class CERMetric(tf.keras.metrics.Metric):\n",
        "    \"\"\"\n",
        "    A custom Keras metric to compute the Character Error Rate\n",
        "    \"\"\"\n",
        "    def __init__(self, name='CER_metric', **kwargs):\n",
        "        super(CERMetric, self).__init__(name=name, **kwargs)\n",
        "        self.cer_accumulator = self.add_weight(name=\"total_cer\", initializer=\"zeros\")\n",
        "        self.counter = self.add_weight(name=\"cer_count\", initializer=\"zeros\")\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        input_shape = K.shape(y_pred)\n",
        "        input_length = tf.ones(shape=input_shape[0]) * K.cast(input_shape[1], 'float32')\n",
        "\n",
        "        decode, log = K.ctc_decode(y_pred,\n",
        "                                    input_length,\n",
        "                                    greedy=True)\n",
        "\n",
        "        decode = K.ctc_label_dense_to_sparse(decode[0], K.cast(input_length, 'int32'))\n",
        "        y_true_sparse = K.ctc_label_dense_to_sparse(y_true, K.cast(input_length, 'int32'))\n",
        "\n",
        "        decode = tf.sparse.retain(decode, tf.not_equal(decode.values, -1))\n",
        "        distance = tf.edit_distance(decode, y_true_sparse, normalize=True)\n",
        "\n",
        "        self.cer_accumulator.assign_add(tf.reduce_sum(distance))\n",
        "        self.counter.assign_add(tf.cast(len(y_true),tf.float32))\n",
        "\n",
        "    def result(self):\n",
        "        return tf.math.divide_no_nan(self.cer_accumulator, self.counter)\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.cer_accumulator.assign(0.0)\n",
        "        self.counter.assign(0.0)\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2,monitor=\"val_CER_metric\",mode=\"min\", restore_best_weights=True)\n",
        "reduce_lr =tf.keras.callbacks.ReduceLROnPlateau(monitor='val_CER_metric', factor=0.2,patience=2,verbose=1, min_delta=0.006,mode=\"min\", min_lr=0.0001)\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\"result.h5\", monitor=\"val_CER_metric\",mode='min', save_best_only=True, save_weights_only=True)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[CERMetric()])\n",
        "\n",
        "model.fit([x_train,y_train],y_train, epochs=30, batch_size=512, validation_split=0.05, callbacks=[early_stopping,model_checkpoint,reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2233bf9c-9daa-47e0-96f4-d97826b3d2a8",
      "metadata": {
        "id": "2233bf9c-9daa-47e0-96f4-d97826b3d2a8"
      },
      "outputs": [],
      "source": [
        "model.evaluate([x_test,y_test], y_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
